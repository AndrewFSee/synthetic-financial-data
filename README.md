# ðŸ¦ Synthetic Financial Data

> Generate realistic synthetic OHLCV (Open, High, Low, Close, Volume) financial time-series data using three state-of-the-art generative model architectures.

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## Overview

**synfin** scaffolds a complete, production-ready Python project for generating synthetic stock market data.  
It implements **three architecture options** you can train, compare, and combine:

| Model | Description | Reference |
|-------|-------------|-----------|
| **TimeGAN** | Recurrent GAN with temporal supervision | Yoon et al., NeurIPS 2019 |
| **Diffusion (DDPM)** | 1D U-Net denoising diffusion model | Ho et al., NeurIPS 2020 |
| **VAE + Copula** | Variational Autoencoder with copula dependency modeling | Kingma & Welling 2014 |

Generated data reproduces key **stylized facts** of financial time series:
- ðŸ“Š Fat-tailed return distributions
- ðŸ“ˆ Volatility clustering
- ðŸ”— Volume-volatility correlation
- ðŸ“‰ Leverage effect

---

## Architecture Diagrams

### TimeGAN (3-Phase Training)

```
Phase 1 â€” Autoencoder:   X â”€â”€â†’ Embedder â”€â”€â†’ H â”€â”€â†’ Recovery â”€â”€â†’ XÌ‚
Phase 2 â€” Supervisor:         H â”€â”€â†’ Supervisor â”€â”€â†’ Åœ
Phase 3 â€” Joint:         Z â”€â”€â†’ Generator â”€â”€â†’ ÃŠ â”€â”€â†’ Supervisor â”€â”€â†’ H_hat
                               Discriminator(H_real vs H_hat) adversarial loss
```

### Diffusion Model (DDPM)

```
Training:   x_0 â”€â”€â†’[add noise t steps]â”€â”€â†’ x_t â”€â”€â†’[UNet1D]â”€â”€â†’ Îµ_pred  (MSE loss vs Îµ)
Sampling:   x_T ~ N(0,I) â”€â”€â†’[denoise T steps]â”€â”€â†’ x_0
```

### VAE + Copula

```
Encoder:  X â”€â”€â†’[LSTM]â”€â”€â†’ (Î¼, ÏƒÂ²)  â”€â”€â†’[reparameterize]â”€â”€â†’ z
Decoder:  z â”€â”€â†’[LSTM]â”€â”€â†’ XÌ‚
Copula:   z_train â”€â”€â†’ Fit Gaussian/Student-t Copula
Generate: Copula.sample() â”€â”€â†’ z â”€â”€â†’ Decoder â”€â”€â†’ X_synthetic
```

---

## Project Structure

```
synthetic-financial-data/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml            # Modern Python project config
â”œâ”€â”€ requirements.txt          # Pinned dependencies
â”œâ”€â”€ setup.cfg                 # Package configuration
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile                  # Common developer commands
â”‚
â”œâ”€â”€ configs/                  # YAML configuration files
â”‚   â”œâ”€â”€ default.yaml          # Shared defaults
â”‚   â”œâ”€â”€ timegan.yaml          # TimeGAN hyperparameters
â”‚   â”œâ”€â”€ diffusion.yaml        # Diffusion model hyperparameters
â”‚   â””â”€â”€ vae_copula.yaml       # VAE+Copula hyperparameters
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Downloaded data (gitignored)
â”‚   â”œâ”€â”€ processed/            # Preprocessed data (gitignored)
â”‚   â””â”€â”€ synthetic/            # Generated synthetic data (gitignored)
â”‚
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_training.ipynb
â”‚   â”œâ”€â”€ 03_evaluation.ipynb
â”‚   â””â”€â”€ 04_visualization.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ synfin/               # Main Python package
â”‚       â”œâ”€â”€ data/             # Data loading & preprocessing
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ timegan/      # TimeGAN (Yoon et al.)
â”‚       â”‚   â”œâ”€â”€ diffusion/    # DDPM diffusion model
â”‚       â”‚   â””â”€â”€ vae_copula/   # VAE + Copula
â”‚       â”œâ”€â”€ training/         # Unified trainer, losses, callbacks
â”‚       â”œâ”€â”€ evaluation/       # Statistical tests, stylized facts, TSTR, privacy
â”‚       â”œâ”€â”€ constraints/      # OHLCV post-processing constraints
â”‚       â”œâ”€â”€ visualization/    # Plotting utilities
â”‚       â””â”€â”€ utils/            # Config, logging, seed, device
â”‚
â”œâ”€â”€ scripts/                  # CLI entry points
â”‚   â”œâ”€â”€ download_data.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ generate.py
â”‚   â””â”€â”€ evaluate.py
â”‚
â””â”€â”€ tests/                    # Unit tests (pytest)
    â”œâ”€â”€ test_data/
    â”œâ”€â”€ test_models/
    â”œâ”€â”€ test_constraints/
    â””â”€â”€ test_evaluation/
```

---

## Installation

### Prerequisites
- Python 3.9+
- pip

### Quick Install

```bash
# Clone the repository
git clone https://github.com/AndrewFSee/synthetic-financial-data.git
cd synthetic-financial-data

# Install dependencies and package
pip install -r requirements.txt
pip install -e .
```

### Developer Install

```bash
pip install -e ".[dev]"
```

Or use the Makefile:

```bash
make install        # Production dependencies
make install-dev    # Development dependencies (includes testing, linting)
```

---

## Quick Start

### 1. Download Financial Data

```bash
# Download daily OHLCV data for multiple tickers
python scripts/download_data.py \
    --tickers AAPL MSFT GOOGL AMZN META \
    --start 2015-01-01 \
    --end 2024-12-31 \
    --interval 1d
```

Or with Make:
```bash
make download
```

### 2. Train a Model

```bash
# Train TimeGAN
python scripts/train.py --model timegan --config configs/timegan.yaml

# Train Diffusion Model
python scripts/train.py --model diffusion --config configs/diffusion.yaml

# Train VAE + Copula
python scripts/train.py --model vae_copula --config configs/vae_copula.yaml
```

```bash
make train-timegan
make train-diffusion
make train-vae
```

### 3. Generate Synthetic Data

```bash
python scripts/generate.py \
    --model timegan \
    --checkpoint checkpoints/timegan_best.pt \
    --num-samples 1000 \
    --seq-length 30
```

```bash
make generate
```

### 4. Evaluate Quality

```bash
python scripts/evaluate.py \
    --real-data data/processed/AAPL.parquet \
    --synthetic-data data/synthetic/timegan_synthetic.npy \
    --output reports/
```

```bash
make evaluate
```

---

## Configuration

All configurations live in `configs/`. The hierarchy is:

```
configs/default.yaml        â† shared base configuration
    â””â”€â”€ configs/timegan.yaml    â† model-specific overrides
    â””â”€â”€ configs/diffusion.yaml
    â””â”€â”€ configs/vae_copula.yaml
```

### Key Configuration Options

```yaml
# configs/default.yaml
data:
  tickers: [AAPL, MSFT, GOOGL]
  start_date: "2015-01-01"
  end_date: "2024-12-31"
  window_size: 30
  normalization: "minmax"   # or "zscore"
  features: [Open, High, Low, Close, Volume, LogReturn, LogVolume, DollarVolume]

training:
  seed: 42
  device: "auto"            # auto-selects CUDA > MPS > CPU
  batch_size: 64
  checkpoint_dir: "checkpoints"
```

---

## Data Module

### Features Computed Automatically

| Feature | Description |
|---------|-------------|
| `LogReturn` | `log(Close_t / Close_{t-1})` |
| `LogVolume` | `log(1 + Volume)` |
| `DollarVolume` | `Close Ã— Volume` |
| `RSI` | Relative Strength Index (14-period) |
| `MACD` | Moving Average Convergence Divergence |
| `BB_Upper/Lower` | Bollinger Bands (20-period, 2Ïƒ) |
| `ATR` | Average True Range (14-period) |
| `RealizedVol` | Rolling std of log returns (annualized) |

---

## Evaluation Methodology

### Statistical Tests
- **Kolmogorov-Smirnov (KS) test** â€” per-feature marginal distribution comparison
- **Maximum Mean Discrepancy (MMD)** â€” distribution distance with RBF kernel
- **ACF comparison** â€” autocorrelation function similarity at multiple lags

### Stylized Facts
- **Fat tails** â€” excess kurtosis of return distributions
- **Volatility clustering** â€” ACF of absolute/squared returns
- **Leverage effect** â€” negative return-volatility correlation
- **Volume-volatility correlation** â€” positive volume and volatility correlation

### TSTR Benchmark (Train on Synthetic, Test on Real)
Trains a downstream classifier (next-day return direction) on synthetic data, evaluates on real data. Compares against TRTR (real-to-real) baseline.

### Privacy Metrics
- **NNDR** â€” Nearest-Neighbor Distance Ratio (memorization detection)
- **DCR** â€” Distance to Closest Record
- **Membership Inference Risk** â€” fraction of synthetic samples suspiciously close to training data

---

## Running Tests

```bash
# Run all tests
make test

# Run specific test module
python -m pytest tests/test_models/test_timegan.py -v
python -m pytest tests/test_constraints/ -v
```

---

## Makefile Reference

| Command | Description |
|---------|-------------|
| `make install` | Install production dependencies |
| `make install-dev` | Install dev dependencies |
| `make download` | Download financial data |
| `make train-timegan` | Train TimeGAN model |
| `make train-diffusion` | Train Diffusion model |
| `make train-vae` | Train VAE+Copula model |
| `make generate` | Generate synthetic data |
| `make evaluate` | Run evaluation suite |
| `make test` | Run unit tests |
| `make lint` | Run flake8, black, isort |
| `make format` | Auto-format code |
| `make clean` | Clean caches and build artifacts |
| `make all` | Full pipeline |

---

## References

1. **TimeGAN**: Yoon, J., Jarrett, D., & van der Schaar, M. (2019). *Time-series Generative Adversarial Networks*. NeurIPS 2019. [arXiv:1906.09592](https://arxiv.org/abs/1906.09592)

2. **DDPM**: Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising Diffusion Probabilistic Models*. NeurIPS 2020. [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)

3. **DDIM**: Song, J., Meng, C., & Ermon, S. (2020). *Denoising Diffusion Implicit Models*. ICLR 2021. [arXiv:2010.02502](https://arxiv.org/abs/2010.02502)

4. **VAE**: Kingma, D.P., & Welling, M. (2014). *Auto-Encoding Variational Bayes*. ICLR 2014. [arXiv:1312.6114](https://arxiv.org/abs/1312.6114)

5. **FinDiff**: Sattarov, T. et al. (2023). *FinDiff: Diffusion Models for Financial Tabular Data Generation*. [arXiv:2309.01472](https://arxiv.org/abs/2309.01472)

6. **Quant GANs**: Wiese, M. et al. (2020). *Quant GANs: Deep Generation of Financial Time Series*. Quantitative Finance. [arXiv:1907.04155](https://arxiv.org/abs/1907.04155)

7. **Stylized Facts**: Cont, R. (2001). *Empirical properties of asset returns: stylized facts and statistical issues*. Quantitative Finance, 1(2), 223â€“236.

---

## License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.